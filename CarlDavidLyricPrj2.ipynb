{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hyeonwid/Lyric_DLProject/blob/main/CarlDavidLyricPrj2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hprsIyablnqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f417c0a6-7cb9-4303-8320-aeead266136d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.11.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "# TODO: change to GPT2 Tokenization Method \n",
        "!pip3 install transformers\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TFGPT2LMHeadModel, AutoConfig, TextStreamer #, AutoModelForCausalLM, AutoTokenizer\n",
        "base_tokenizer = GPT2Tokenizer.from_pretrained('gpt2') #AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "base_model = GPT2LMHeadModel.from_pretrained('gpt2') #AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "# from transformers import BertTokenizer\n",
        "# from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "# from keras.utils import to_categorical\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.text import text_to_word_sequence\n",
        "# from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Early on experiment with bert tokenizer and fine tuning it\n",
        "\n",
        "# bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# tokenizer = BertWordPieceTokenizer(\n",
        "#     clean_text=True,\n",
        "#     handle_chinese_chars=False,\n",
        "#     strip_accents=False,\n",
        "#     lowercase=False\n",
        "# )\n",
        "# tokenizer.train_from_iterator(X_train, vocab_size=30_000, min_frequency=2,\n",
        "#                 limit_alphabet=1000, wordpieces_prefix='##',\n",
        "#                 special_tokens=[\n",
        "#                     '[PAD', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])\n",
        "# import os\n",
        "# os.mkdir('./bert-it')\n",
        "# tokenizer.save_model('./bert-it', 'bert-it')\n",
        "# tokenizer = BertTokenizer.from_pretrained('./bert-it/bert-it-vocab.txt')"
      ],
      "metadata": {
        "id": "ittk-B65D2z1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset features are \"Artist\" \"Song\" \"Lyrics\"\n",
        "dataset = load_dataset(\"chloeliu/lyrics\", split='train') # this dataset already removes \"the, a ...\"\" from lyrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIxyNzOfDZVO",
        "outputId": "a21d22ed-c8e3-418d-fbfb-c184e5c14d07"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/chloeliu___csv/chloeliu--lyrics-e9993b0bb78e7dda/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploring dataset\n",
        "# alphabet = set(' '.join(dataset['train']['lyrics']))\n",
        "# artists = np.unique(dataset['train']['artist_name'])\n",
        "# print('billie eilish' in artists)"
      ],
      "metadata": {
        "id": "x4rfFZbNRr1k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y=dataset['artist_name']\n",
        "print(len(Y))\n",
        "num_classes = len(np.unique(Y))\n",
        "print(num_classes)\n",
        "\n",
        "text = dataset['lyrics']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsxsNuviI_b7",
        "outputId": "e998015d-98a2-4729-bad8-1166112104cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28372\n",
            "5426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the eos and bos tokens are defined\n",
        "bos = '<|endoftext|>'\n",
        "eos = '<|EOS|>'\n",
        "pad = '<|pad|>'\n",
        "\n",
        "special_tokens_dict = {'eos_token': eos, 'bos_token': bos, 'pad_token': pad}\n",
        "\n",
        "# the new token is added to the tokenizer\n",
        "num_added_tokens = base_tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "config = AutoConfig.from_pretrained('gpt2', \n",
        "                                    bos_token_id=base_tokenizer.bos_token_id,\n",
        "                                    eos_token_id=base_tokenizer.eos_token_id,\n",
        "                                    pad_token_id=base_tokenizer.pad_token_id,\n",
        "                                    output_hidden_states=False)\n",
        "\n",
        "# the pre-trained model is loaded with the custom configuration\n",
        "base_model = GPT2LMHeadModel.from_pretrained('gpt2', config=config)\n",
        "\n",
        "# the model embedding is resized\n",
        "base_model.resize_token_embeddings(len(base_tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9zUy-uvr4uB",
        "outputId": "651dcecd-bc20-409e-9464-e6ba5bb3a676"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50259, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = [bos + ' ' + s + ' ' + eos for s in text]"
      ],
      "metadata": {
        "id": "oWII3uHosT4K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = 0.8\n",
        "test_split = 0.1\n",
        "\n",
        "train_ind = int(train_split * (len(dataset)/10))\n",
        "test_ind = int((1 - test_split) * (len(dataset)/10))\n",
        "\n",
        "# X_train = (new_text[:train_ind])\n",
        "# X_valid = (new_text[train_ind:test_ind])\n",
        "# X_test = (new_text[test_ind:])"
      ],
      "metadata": {
        "id": "w-26X-IpIh6-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the text to convert them into numerical inputs that can be fed into the deep learning model. This involves breaking up the text into individual words or subwords, and mapping each token to a unique integer. The tokenization method must be aligned with the chosen model."
      ],
      "metadata": {
        "id": "o4CXHFT5Tsaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "        return base_tokenizer(examples['lyrics'], padding=True)\n",
        "\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    num_proc=5,\n",
        "    remove_columns=list(dataset.features.keys()),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8oPRDEstANs",
        "outputId": "993d3f15-9978-44a1-ac70-4236d6fa765d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/chloeliu___csv/chloeliu--lyrics-e9993b0bb78e7dda/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-8b971a66949edb7e_*_of_00005.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_tokenizer.decode(tokenized_dataset['input_ids'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "ignfqALRx5Dy",
        "outputId": "7f5c9002-6a74-4533-ecd4-61039887f218"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hold time feel break feel untrue convince speak voice tear try hold hurt try forgive okay play break string feel heart want feel tell real truth hurt lie worse anymore little turn dust play house ruin run leave save like chase train late late tear try hold hurt try forgive okay play break string feel heart want feel tell real truth hurt lie worse anymore little run leave save like chase train know late late play break string feel heart want feel tell real truth hurt lie worse anymore little know little hold time feel <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|> <|pad|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # split tokenized data into training validation and test\n",
        "# tokenized_X_train = (tokenized_dataset[:train_ind])\n",
        "# tokenized_X_valid = (tokenized_dataset[train_ind:test_ind])\n",
        "# tokenized_X_test = (tokenized_dataset[test_ind:])"
      ],
      "metadata": {
        "id": "u9RlqOrR1ESx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Correct Milestone 1\n",
        "*   Train Model\n",
        "*   Evaluate model on test set\n",
        "*   Update readme with instructions how to run code\n",
        "*   Add comments to code where needed"
      ],
      "metadata": {
        "id": "_iTAyHls8TZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, DataCollatorForLanguageModeling, Trainer\n",
        "\n",
        "model_headlines_path = './output'\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": accuracy}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_headlines_path,          # output directory\n",
        "    num_train_epochs=6,              # total # of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=4,   # batch size for evaluation\n",
        "    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir=model_headlines_path,            # directory for storing logs\n",
        "    prediction_loss_only=True,\n",
        "    save_steps=10000,\n",
        "    gradient_accumulation_steps=4\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=base_tokenizer,\n",
        "        mlm=False\n",
        "    )"
      ],
      "metadata": {
        "id": "t50Ms-s707D2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator([tokenized_dataset[i] for i in range(5)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9360P7aL-qUw",
        "outputId": "d77b3dc3-9f48-4e71-ed73-0243a1934772"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 2946,   640,  1254,  ..., 50258, 50258, 50258],\n",
              "        [ 6667, 12311,  4268,  ..., 50258, 50258, 50258],\n",
              "        [34751, 11499,  3758,  ..., 50258, 50258, 50258],\n",
              "        [41304, 11914,   765,  ..., 50258, 50258, 50258],\n",
              "        [   83,   359, 40003,  ..., 50258, 50258, 50258]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ 2946,   640,  1254,  ...,  -100,  -100,  -100],\n",
              "        [ 6667, 12311,  4268,  ...,  -100,  -100,  -100],\n",
              "        [34751, 11499,  3758,  ...,  -100,  -100,  -100],\n",
              "        [41304, 11914,   765,  ...,  -100,  -100,  -100],\n",
              "        [   83,   359, 40003,  ...,  -100,  -100,  -100]])}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples = [tokenized_dataset[i] for i in range(train_ind)]\n",
        "valid_samples = [tokenized_dataset[i] for i in range(train_ind, test_ind)]\n",
        "test_samples = [tokenized_dataset[i] for i in range(test_ind, len(tokenized_dataset))]"
      ],
      "metadata": {
        "id": "7qYYvGCB_b4v"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=base_model,                         # the instantiated ü§ó Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_samples,         # training dataset\n",
        "    eval_dataset=valid_samples,\n",
        "    compute_metrics=compute_metrics         # evaluation dataset\n",
        ")\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "\n",
        "#OutOfMemoryError: CUDA out of memory. Tried to allocate 216.00 MiB (GPU 0; 14.75 GiB total capacity; 13.61 GiB already allocated; 198.81 MiB free; 13.78 GiB reserved in total by PyTorch) \n",
        "#If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "TsT0lNHc-oNB",
        "outputId": "515f9505-bdb8-4147-bc3a-0eda063b157c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='426' max='426' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [426/426 18:06, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='71' max='71' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [71/71 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 4.527557373046875,\n",
              " 'eval_runtime': 7.3235,\n",
              " 'eval_samples_per_second': 38.779,\n",
              " 'eval_steps_per_second': 9.695,\n",
              " 'epoch': 6.0}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UwBc9ENW060p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this was to calculate how many words appeared more than 10 times\n",
        "\n",
        "# from collections import defaultdict\n",
        "\n",
        "# # Store word occurrences\n",
        "# d = defaultdict(int)\n",
        "# for phrase in text:\n",
        "#   for word in phrase.split(' '):\n",
        "#     d[word] += 1\n",
        "\n",
        "# # top = (np.argmax(list(d.values())))\n",
        "# # print(list(d.keys())[top])\n",
        "# # print(list(d.values())[top])\n",
        "\n",
        "# # Calculate number of words that appear 10 or more times\n",
        "# num_words = 0\n",
        "# for val in list(d.values()):\n",
        "#   if val >= 10: \n",
        "#     num_words += 1\n",
        "# print(num_words)"
      ],
      "metadata": {
        "id": "gwvzXo4X5AhX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # TODO: change to BERT Tokenization Method\n",
        "\n",
        "# # Update: limit wordindex to only words with 10 or more appearances\n",
        "# tokenizer = Tokenizer(num_words=num_words, oov_token=\"<OOV>\")\n",
        "# # Update: only the training data is used for the calculation of word indices\n",
        "# tokenizer.fit_on_texts(X_train)\n",
        "# seq = tokenizer.texts_to_sequences(text)\n",
        "# print(seq[0])\n",
        "# Xmat = tokenizer.sequences_to_matrix(seq, mode=\"tfidf\")\n",
        "# print(Xmat[0])\n",
        "# print(len(Xmat) == len(X_train))"
      ],
      "metadata": {
        "id": "t0EB4ClJm5oa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output = bert_tokenizer.tokenize(' '.join(X_train))"
      ],
      "metadata": {
        "id": "ni8doJdrfcsj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Familiarizing with gpt2 model and generating text\n",
        "\n",
        "# text = \"Replace me by any text you'd like.\"\n",
        "# outputs = base_model.generate(**base_tokenizer(text, return_tensors=\"tf\"))\n",
        "\n",
        "# texts = [text, '„Å°', 'I love Star Wars']\n",
        "# encoded_input = [base_tokenizer.encode(text, return_tensors='tf') for text in texts]\n",
        "# outputs = [base_model.generate(e) for e in encoded_input]\n",
        "\n",
        "# for output in outputs:\n",
        "#   print(base_tokenizer.decode(output)) #output[0]\n",
        "#   print('----------')"
      ],
      "metadata": {
        "id": "py6tGHsT5ZfF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Not sure where labels fit in above\n",
        "\n",
        "# Y_ind = range(num_classes)\n",
        "# d = dict(zip(np.unique(Y), Y_ind))\n",
        "# for i in range(len(Y)):\n",
        "#   Y[i] = d[Y[i]]\n",
        "  \n",
        "# Y_train = np.array(Y[:train_ind])\n",
        "# Y_valid = np.array(Y[train_ind:test_ind])\n",
        "# Y_test = np.array(Y[test_ind:])\n",
        "\n",
        "# print(X_train.shape) # input shape (num_words,)\n",
        "# Y_train = to_categorical(Y_train,num_classes=num_classes)\n",
        "# Y_valid = to_categorical(Y_valid,num_classes=num_classes)\n",
        "# Y_test = to_categorical(Y_test,num_classes=num_classes)\n",
        "\n",
        "# Y_test[:5]\n",
        "\n",
        "# X_train = text[:train_ind]\n",
        "# X_train = np.array(Xmat[:train_ind])\n",
        "# X_valid = np.array(Xmat[train_ind:test_ind])\n",
        "# X_test = np.array(Xmat[test_ind:])"
      ],
      "metadata": {
        "id": "dn6xvoyqLbFx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "af-PhOzI9EdD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# should we standardize (mean/std)\n",
        "mean = np.mean(X_train, axis=0)\n",
        "std = np.std(X_train, axis=0)\n",
        "X_train = (X_train - mean) / std\n",
        "X_valid = (X_valid - mean) / std\n",
        "X_test = (X_test - mean) / std\n",
        "\n",
        "# RuntimeWarning: invalid value encountered in true_divide"
      ],
      "metadata": {
        "id": "DNKnCQ-KLWYS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "f83cff9f-bb38-4103-b1a5-552983c0b6ec"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a665e4118e03>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# should we standardize (mean/std)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ignore: stop words nltk testing"
      ],
      "metadata": {
        "id": "0YRUGGEgMizt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# # from nltk.stem import PorterStemmer\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')"
      ],
      "metadata": {
        "id": "JXd7toNKx90J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example_sent = \"\"\"This is a sample sentence,\n",
        "#                   showing off the stop words filtration.\"\"\"\n",
        "  \n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# # ps = PorterStemmer()\n",
        "# # use with ps.stem(str)\n",
        "\n",
        "# word_tokens = word_tokenize(example_sent)\n",
        "# filtered_sentence = [w for w in word_tokens if w not in stop_words]\n",
        "# print(word_tokens)\n",
        "# print(filtered_sentence)"
      ],
      "metadata": {
        "id": "WthyKBpKngtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sa-WqrEeH8lC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}